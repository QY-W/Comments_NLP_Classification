{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93c6cbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.8 (main, Oct 13 2022, 09:48:40) [Clang 14.0.0 (clang-1400.0.29.102)]\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2d4cc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/qiyuan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/qiyuan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/qiyuan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/qiyuan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'entropy' from 'scipy.stats' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124momw-1.4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvaderSentiment\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvaderSentiment\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentimentIntensityAnalyzer\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpora\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcorpora\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpora\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dictionary\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/gensim/__init__.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/gensim/corpora/__init__.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexedcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleicorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/gensim/corpora/indexedcorpus.py:14\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[1;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIndexedCorpus\u001b[39;00m(interfaces\u001b[38;5;241m.\u001b[39mCorpusABC):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/gensim/interfaces.py:19\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[1;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCorpusABC\u001b[39;00m(utils\u001b[38;5;241m.\u001b[39mSaveLoad):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/gensim/matutils.py:19\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m entropy\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_blas_funcs, triu\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlapack\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_lapack_funcs\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'entropy' from 'scipy.stats' (unknown location)"
     ]
    }
   ],
   "source": [
    "# python version 3.8.6rc1\n",
    "import pandas as pd\n",
    "import string\n",
    "import util\n",
    "import io\n",
    "import os\n",
    "from pprint import pprint\n",
    "import spacy\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "#warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4180e9",
   "metadata": {},
   "source": [
    "# Saving clean data in pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "091b3b1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\willi\\miniconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>harmful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\r\\nWhy the edits made under my use...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\r\\nMore\\r\\nI can't make any real suggestions...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  harmful\n",
       "0  Explanation\\r\\nWhy the edits made under my use...        0\n",
       "1  D'aww! He matches this background colour I'm s...        0\n",
       "2  Hey man, I'm really not trying to edit war. It...        0\n",
       "3  \"\\r\\nMore\\r\\nI can't make any real suggestions...        0\n",
       "4  You, sir, are my hero. Any chance you remember...        0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comments = pd.read_csv(\"train.csv\")\n",
    "# comments = pd.read_csv(\"train.csv\")\n",
    "\n",
    "\n",
    "# comments['harmful'] = comments.iloc[:, 2:].sum(axis=1)\n",
    "\n",
    "\n",
    "# df = comments[['comment_text','harmful']]\n",
    "\n",
    "# # print merged label \n",
    "# print(df.groupby(['harmful']).count())\n",
    "\n",
    "\n",
    "# df.loc[df['harmful'] != 0, 'harmful'] = 1\n",
    "\n",
    "# # print merged label changed to ones \n",
    "# print(df.groupby(['harmful']).count())\n",
    "\n",
    "\n",
    "\n",
    "# df_harmful = df[df['harmful'] > 0] \n",
    "# df_not_harmful = df[df['harmful'] == 0]\n",
    "# # check label fraction\n",
    "comments = pd.read_csv(\"train.csv\")\n",
    "comments['harmful'] = comments.iloc[:, 2:].sum(axis=1)\n",
    "\n",
    "\n",
    "\n",
    "df = comments[['comment_text',\"harmful\"]]#.copy()\n",
    "\n",
    "df.loc[df['harmful'] != 0, 'harmful'] = 1\n",
    "df_harmful = df[df['harmful'] > 0] \n",
    "df_not_harmful = df[df['harmful'] == 0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "baa47c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    143346\n",
      "1     16225\n",
      "Name: harmful, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['harmful'].value_counts())\n",
    "\n",
    "# Randomly select 15% of not harmful instances + all harmful instances\n",
    "    #whole data result into dead kernel\n",
    "    #and unbalanced data may result into potential problem \n",
    "\n",
    "df_not_harmful_sample = df_not_harmful.sample(frac=0.15)\n",
    "sample_df = pd.concat([df_not_harmful_sample, df_harmful], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0067acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>harmful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105447</th>\n",
       "      <td>I contest the deletion of this page because it...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19627</th>\n",
       "      <td>Yes, the ,  and such may be better candidates....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77114</th>\n",
       "      <td>GOCE 2011 Year-End Report\\r\\n \\r\\nSent on beha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36416</th>\n",
       "      <td>\"\\r\\nBut seriously\\r\\nI do commend you managin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96583</th>\n",
       "      <td>What it means is that the description is missi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159494</th>\n",
       "      <td>\"\\r\\n\\r\\n our previous conversation \\r\\n\\r\\nyo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159514</th>\n",
       "      <td>YOU ARE A MISCHIEVIOUS PUBIC HAIR</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159541</th>\n",
       "      <td>Your absurd edits \\r\\n\\r\\nYour absurd edits on...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159546</th>\n",
       "      <td>\"\\r\\n\\r\\nHey listen don't you ever!!!! Delete ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159554</th>\n",
       "      <td>and i'm going to keep posting the stuff u dele...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37727 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text  harmful\n",
       "105447  I contest the deletion of this page because it...        0\n",
       "19627   Yes, the ,  and such may be better candidates....        0\n",
       "77114   GOCE 2011 Year-End Report\\r\\n \\r\\nSent on beha...        0\n",
       "36416   \"\\r\\nBut seriously\\r\\nI do commend you managin...        0\n",
       "96583   What it means is that the description is missi...        0\n",
       "...                                                   ...      ...\n",
       "159494  \"\\r\\n\\r\\n our previous conversation \\r\\n\\r\\nyo...        1\n",
       "159514                  YOU ARE A MISCHIEVIOUS PUBIC HAIR        1\n",
       "159541  Your absurd edits \\r\\n\\r\\nYour absurd edits on...        1\n",
       "159546  \"\\r\\n\\r\\nHey listen don't you ever!!!! Delete ...        1\n",
       "159554  and i'm going to keep posting the stuff u dele...        1\n",
       "\n",
       "[37727 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb438ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_df_copy = sample_df.copy()\n",
    "df_harmful_copy = df_harmful.copy()\n",
    "\n",
    "with open('pkls/data_sampled.pkl', \"wb\") as f:\n",
    "    pickle.dump(sample_df_copy, f)\n",
    "\n",
    "with open('pkls/data_harmful.pkl', \"wb\") as f:\n",
    "    pickle.dump(df_harmful_copy, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3cd91d",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8098432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    def remove_punctuation(text):\n",
    "        #return str(text).translate(str.maketrans('', '', string.punctuation))\n",
    "        text_nopunct = \"\".join([char if char not in string.punctuation else ' ' for char in str(text)])\n",
    "        return text_nopunct\n",
    "\n",
    "    def tokenize_text(text):\n",
    "        tokens = nltk.word_tokenize(text) \n",
    "        tokens = [token.strip() for token in tokens]\n",
    "        return tokens\n",
    "\n",
    "    def remove_stopwords(tokens):\n",
    "        stopword_list = nltk.corpus.stopwords.words('english')\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "        #filtered_text = ' '.join(filtered_tokens)    \n",
    "        return filtered_tokens\n",
    "\n",
    "    def expand_contractions(text, contraction_mapping):\n",
    "\n",
    "        contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                          flags=re.IGNORECASE|re.DOTALL)\n",
    "        def expand_match(contraction):\n",
    "            match = contraction.group(0)\n",
    "            first_char = match[0]\n",
    "            expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                    if contraction_mapping.get(match)\\\n",
    "                                    else contraction_mapping.get(match.lower())                       \n",
    "            expanded_contraction = first_char+expanded_contraction[1:]\n",
    "            return expanded_contraction\n",
    "\n",
    "        expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "        expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "        return expanded_text\n",
    "    \n",
    "    def remove_digit(text):\n",
    "        text_nodigit = re.sub(r'\\w*\\d\\w*', '',text).strip()\n",
    "        return text_nodigit\n",
    "\n",
    "    def stemming(tokenized_text):\n",
    "        ps = nltk.PorterStemmer()\n",
    "        stemmed = [ps.stem(word) for word in tokenized_text]\n",
    "        return stemmed\n",
    "\n",
    "    def lemmatize(tokenized_text):\n",
    "        wn = nltk.WordNetLemmatizer()\n",
    "        lemmatized = [wn.lemmatize(word) for word in tokenized_text]\n",
    "        return lemmatized\n",
    "    \n",
    "    \n",
    "    df[\"expand_contractions\"] = df[\"comment_text\"].apply(lambda x: expand_contractions(x.lower(), util.contraction_mapping))\n",
    "    df[\"remove_punctuation\"] = df[\"expand_contractions\"].apply(lambda x: remove_punctuation(x))\n",
    "    df[\"remove_digit\"] = df[\"remove_punctuation\"].apply(lambda x: remove_digit(x))\n",
    "    df[\"tokenized\"] = df[\"remove_digit\"].apply(lambda x: tokenize_text(x))\n",
    "    df['remove_stopwords'] =  df[\"tokenized\"].apply(lambda x: remove_stopwords(x))\n",
    "    df['stemmed'] =  df[\"remove_stopwords\"].apply(lambda x: stemming(x))\n",
    "    df['lemmatized'] =  df[\"stemmed\"].apply(lambda x: lemmatize(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32104a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_harmful_copy2 = df_harmful.copy()\n",
    "df_harmful_preprocessed = preprocess(df_harmful_copy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1be045eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled_preprocessed = preprocess(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d519134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(df_sampled_preprocessed, open('pkls/data_sampled_processed.pkl', 'wb'))\n",
    "# pickle.dump(df_harmful_preprocessed, open('pkls/data_harmful_processed.pkl', 'wb'))\n",
    "\n",
    "with open('pkls/data_sampled_processed.pkl', \"wb\") as f:\n",
    "    pickle.dump(df_sampled_preprocessed, f)\n",
    "\n",
    "with open('pkls/data_harmful_processed.pkl', \"wb\") as f:\n",
    "    pickle.dump(df_harmful_preprocessed, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
