{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93c6cbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.5 (default, May 18 2021, 14:42:02) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "695b081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical, pad_sequences\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2d4cc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Willi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Willi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Willi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Willi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# python version 3.8.6rc1\n",
    "import pandas as pd\n",
    "import string\n",
    "import util\n",
    "import io\n",
    "import os\n",
    "from pprint import pprint\n",
    "import spacy\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "#warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "# import pyLDAvis\n",
    "# import pyLDAvis.gensim_models\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib\n",
    "# %matplotlib inline\n",
    "# import pandas as pd\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn import metrics\n",
    "# from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66360c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    143346\n",
      "1     16225\n",
      "Name: harmful, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Willi\\AppData\\Local\\Temp/ipykernel_22988/278609579.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  comments['harmful'][comments['harmful']>0] = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Number of harmful messages')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count number of rows with each label without considering combination\n",
    "comments = pd.read_csv(\"train.csv\")\n",
    "#comments.iloc[:, 2:].sum(axis=0)\n",
    "\n",
    "df_multilabel = comments\n",
    "\n",
    "# convert into binary\n",
    "comments['harmful'] = comments.iloc[:, 2:].sum(axis=1)\n",
    "comments['harmful'][comments['harmful']>0] = 1\n",
    "\n",
    "# keep necessary info\n",
    "df = comments[['comment_text',\"harmful\"]]#.copy()\n",
    "\n",
    "print(df['harmful'].value_counts())\n",
    "df.head()\n",
    "\n",
    "sns.countplot(df['harmful'])\n",
    "plt.xlabel('Label')\n",
    "plt.title('Number of harmful messages')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3cd91d",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8098432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    def remove_punctuation(text):\n",
    "        #return str(text).translate(str.maketrans('', '', string.punctuation))\n",
    "        text_nopunct = \"\".join([char for char in str(text) if char not in string.punctuation])\n",
    "        return text_nopunct\n",
    "\n",
    "    def tokenize_text(text):\n",
    "        tokens = nltk.word_tokenize(text) \n",
    "        tokens = [token.strip() for token in tokens]\n",
    "        return tokens\n",
    "\n",
    "    def remove_stopwords(tokens):\n",
    "        stopword_list = nltk.corpus.stopwords.words('english')\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "        #filtered_text = ' '.join(filtered_tokens)    \n",
    "        return filtered_tokens\n",
    "\n",
    "    def expand_contractions(text, contraction_mapping):\n",
    "\n",
    "        contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                          flags=re.IGNORECASE|re.DOTALL)\n",
    "        def expand_match(contraction):\n",
    "            match = contraction.group(0)\n",
    "            first_char = match[0]\n",
    "            expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                    if contraction_mapping.get(match)\\\n",
    "                                    else contraction_mapping.get(match.lower())                       \n",
    "            expanded_contraction = first_char+expanded_contraction[1:]\n",
    "            return expanded_contraction\n",
    "\n",
    "        expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "        expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "        return expanded_text\n",
    "    def remove_digit(text):\n",
    "        text_nodigit = re.sub(r'\\w*\\d\\w*', '',text).strip()\n",
    "        return text_nodigit\n",
    "\n",
    "    def stemming(tokenized_text):\n",
    "        ps = nltk.PorterStemmer()\n",
    "        stemmed = [ps.stem(word) for word in tokenized_text]\n",
    "        return stemmed\n",
    "\n",
    "    def lemmatize(tokenized_text):\n",
    "        wn = nltk.WordNetLemmatizer()\n",
    "        lemmatized = [wn.lemmatize(word) for word in tokenized_text]\n",
    "        return lemmatized\n",
    "    # expand_contractions -> remove_punctuation -> remove_digit -> tokenize_text -> remove_stopwords\n",
    "    \n",
    "    df[\"comment_text_clean\"] = df[\"comment_text\"].apply(lambda x: expand_contractions(x.lower(), util.contraction_mapping))\n",
    "    df[\"comment_text_clean\"] = df[\"comment_text_clean\"].apply(lambda x: remove_punctuation(x))\n",
    "    df[\"comment_text_clean\"] = df[\"comment_text_clean\"].apply(lambda x: remove_digit(x))\n",
    "    #df[\"tokenized\"] = df[\"comment_text_clean\"].apply(lambda x: tokenize_text(x))\n",
    "    #df['tokenized'] =  df[\"tokenized\"].apply(lambda x: remove_stopwords(x))\n",
    "    #df['stemmed'] =  df[\"tokenized\"].apply(lambda x: stemming(x))\n",
    "    #df['lemmatized'] =  df[\"tokenized\"].apply(lambda x: lemmatize(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee367bde",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Willi\\AppData\\Local\\Temp/ipykernel_22988/3222826130.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"comment_text_clean\"] = df[\"comment_text\"].apply(lambda x: expand_contractions(x.lower(), util.contraction_mapping))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>harmful</th>\n",
       "      <th>comment_text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation\\nwhy the edits made under my usern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>daww he matches this background colour i am se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man i am really not trying to edit war it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>more\\ni cannot make any real suggestions on im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  harmful  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...        0   \n",
       "1  D'aww! He matches this background colour I'm s...        0   \n",
       "2  Hey man, I'm really not trying to edit war. It...        0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...        0   \n",
       "4  You, sir, are my hero. Any chance you remember...        0   \n",
       "\n",
       "                                  comment_text_clean  \n",
       "0  explanation\\nwhy the edits made under my usern...  \n",
       "1  daww he matches this background colour i am se...  \n",
       "2  hey man i am really not trying to edit war it ...  \n",
       "3  more\\ni cannot make any real suggestions on im...  \n",
       "4  you sir are my hero any chance you remember wh...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = preprocess(df)\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d5c2c9",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f86a903c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_clean['comment_text_clean']\n",
    "#X = df_clean['comment_text']\n",
    "Y = df_clean['harmful']\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99c428d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_WORDS = 2000\n",
    "# Max number of words in each complaint.\n",
    "MAX_LENGTH = 200\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_matrix = pad_sequences(sequences,maxlen=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d99cd3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[MAX_LENGTH])\n",
    "    layer = Embedding(MAX_WORDS,50,input_length=MAX_LENGTH)(inputs)\n",
    "    layer = LSTM(64)(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "349af818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inputs (InputLayer)         [(None, 200)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 200, 50)           100000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                29440     \n",
      "                                                                 \n",
      " FC1 (Dense)                 (None, 256)               16640     \n",
      "                                                                 \n",
      " activation (Activation)     (None, 256)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " out_layer (Dense)           (None, 1)                 257       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 146,337\n",
      "Trainable params: 146,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b774c77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "848/848 [==============================] - 90s 105ms/step - loss: 0.1730 - accuracy: 0.9423 - val_loss: 0.1330 - val_accuracy: 0.9547\n",
      "Epoch 2/10\n",
      "848/848 [==============================] - 88s 103ms/step - loss: 0.1348 - accuracy: 0.9546 - val_loss: 0.1322 - val_accuracy: 0.9554\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(sequences_matrix,Y_train,batch_size=128,epochs=10,\n",
    "          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03a05ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "748/748 [==============================] - 10s 13ms/step - loss: 0.1300 - accuracy: 0.9542\n"
     ]
    }
   ],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = pad_sequences(test_sequences,maxlen=MAX_LENGTH)\n",
    "accr = model.evaluate(test_sequences_matrix,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e813bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set\n",
      "  Loss: 0.130\n",
      "  Accuracy: 0.954\n"
     ]
    }
   ],
   "source": [
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
